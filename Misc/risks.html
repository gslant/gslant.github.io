<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>Risks</title>
    <link rel="stylesheet" href="../styles/mystyles.css">
  </head>

<body>
<img src="../images/csg4ed-small.png" alt="Enhancing Social Good in Computing"
<!-- Site navigation menu -->

<ul class="navbar">
  <li><a href="../index.html">Home page</a>
  <li><a href="topic.html">Technology/Topic</a>
  <li><a href="opportunities.html">Opportunities</a>
  <li><a href="misc/risks.html">Risks</a>
  <li><a href="choices.html">Choices</a>
  <li><a href="ethics.html">Ethical Reflections</a>
  <li><a href="references.html">References</a>
   <li><a href="process.html">Process Support</a>
</ul>

<!-- Main content -->
<h1>Technology Risks</h1>

  

</ul><h4><p> The central theme of this study is to explore the possible/current risks AI delivers as it is only in its early development stages. This can mean that there are occasions where the technology can act upon false alarms such as misleading situations, or evolving to a bias state. Though there is a library of possible risks to mention, three aspects will be further examined for this report. The risks investigated include privacy threats of AI surveillance, improper use of AI by the government, and future societal issues influenced by AI surveillance.
<p></h4> 
<br>
<h4><p>Firstly, society criticizes the intelligence of AI that is implemented into surveillance systems. Given that AI is only in its early stages, the threats it poses to our privacy are a major risk and must be addressed. In a YouTube video presented by ACLU, the technology surveillance systems use is called ‘visual analytics’, where the computer is able to understand and judge while recording. From here, the AI is able to learn and collect data about individuals by color, certain clothing, size, gender, and many more categories. Anyone who fits under the existing categories will be recognized by the AI and the data collected is saved and stored for playback or analysis by authorities (Stanley, 2019). However, this technology is not only active in the U.S. but also in governments abroad such as Australia, Brazil, China, and several European countries.
China is considered to be one of the major providers of AI surveillance technologies (Feldstein, 2019). Its use of surveillance over the Muslim-majority Uighur population in the Xinjiang region is intensely monitored by government authorities. Officials in China use facial recognition based on AI in order to track Uighurs based on facial appearance (Luco & Muyskens, n.d.). The Uighurs are prone to the control of surveillance systems and their privacy is being invaded. At this point, the plight of Chinese Uighurs carries an ongoing state of insecurity as their government stripped them of their privacy. People's privacy is a universal human right, the right to privacy is an essential human right, this is presented under Article 12 of the Universal Declaration on Human Rights and Article 17 of the ICCPR (New Zealand Human Rights Commission, 2018, #8). Due to governments possessing powerful control over surveillance, this links to the second circumstance where ‘privacy threats are determined by the government’s methods of AI use. With this in mind, would authorities find it necessary to reduce the right to privacy for the sake of security and safety?
</p></h4>
<br>
<h4><p>Secondly, these privacy issues can be influenced by the government’s improper use of this technology. The government itself is capable of misusing and taking advantage of this technology for its benefit. Generally, governments use AI surveillance to predict any upcoming conflict or threats that cause harm to the community. However, some governments use them for their gain to accomplish political objectives. As explained by Steve Feldstein, governments in liberal democracies are less likely to abuse the use of surveillance than governments in autocratic (China, Russia, Saudi Arabia) and semi-autocratic countries. These countries’ primary use of AI is to utilize mass surveillance purposes. While governments possessing dismal human rights records use AI surveillance in such ways to reinforce restraints (Feldstein, 2019). As mentioned previously, AI surveillance is used against the Uighurs, a vulnerable community in the Xinjiang region, China. Similarly, communities in other countries besides China are strongly monitored under AI surveillance. Ali (2016, pp. 78 - 95) reported that after the tragic event of 9/11, the Muslim community in New York is closely monitored by the New York Police Department (NYPD). The Associated Press (AP) revealed secret NYPD documents that planned a regional program. This was designed to “put American citizens under surveillance and examined where they ate, prayed, and worked solely because of their ethnicity” as was found (Associated Press Interactive, 2012, as cited in Ali, 2016.) Closely spying on the Muslim community due to their ethnicity is abusing the advancement’s power against a certain group in American society. This issue falls under a bias state and most definitely leads to future concerns that can spread across America and the globe. Being able to Does the implementation of AI in security really prioritize safety at this point?
</p></h4>
<br>
<h4><p>Lastly, the government’s approach to the use of these intelligent eyes shapes the future of society. With bias and privacy issues in the atmosphere, the future is at risk and the present becomes concerning. The power to identify people and patterns under the authorities’ control will change the relationship between the citizens and the government. With the AI technology implemented in surveillance systems improving drastically, the data collected by these computers can reveal much of an individual’s identity. Xie Yinan (VICE News et al., 2018/2019) suggests that with the current AI advancements improving day by day, the future can look like the television series ‘Black Mirror’.
According to Julia Dzurillay, Black Mirror is one of Netflix’s most popular original series and explores the sinister side of technology. Each episode presents a diverse story of how social media and technology consume our lives. The title, Balck Mirror, applies to how screens of devices are seen when they are off (Dzurillay, 2019). Throughout the film, names, social scores, and a photo of the individual can be accessed by anyone through a device. This demonstrates how our modern-day advancements collect data, how it can be used, and how it poses a major threat to our privacy.
However, the future Xie Yinan describes is not too far away. An article by Nicole Kobie found that China’s rising social credit system is being compared to the series Black Mirror. The reality is much more complicated and rather worse. The system is can be used to judge citizens’ behaviour and trustworthiness. Being caught jaywalking or not paying a court bill can result in losing rights to book a flight or get a train ticket (Kobie, 2019). The concept method of the score system is not only invading other rights, but also causing the government to be very manipulative of its citizens. 
</p></h4>
<br>
 
<p><em>thanks to W3C for tutorial and adapted code from <a href="https://www.w3.org/Style/Examples/011/firstcss.en.html">Style Examples</a></p></em>
<p><em>also thanks to WDN for HTML and CSS resources and any adapted code snippets from <a href= "https://developer.mozilla.org/en-US/docs/Web">Mozilla Developer Network</a></p></em> 
 </html>
